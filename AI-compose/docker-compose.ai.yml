services:
  ollama:
    image: ollama/ollama:latest
    container_name: ${BASE_CONTAINER_NAME}ollama
    volumes:
      - ${AI_CONFIG_ROOT}/ollama:/root/.ollama
      - ${AI_DATA_ROOT}/ollama-models:/root/.ollama/models
    tty: true
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: ["gpu"]
            count: all
    restart: always

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ${BASE_CONTAINER_NAME}open-webui
    volumes:
      - ${AI_CONFIG_ROOT}/open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT}:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY:-}'
    extra_hosts:
      - host.docker.internal:host-gateway
    labels:
      - traefik.${BASE_CONTAINER_NAME}=true
      - traefik.enable=true
      - traefik.http.routers.open-webui.rule=Host(`ai.${DOMAIN}`)
      - traefik.http.services.open-webui.loadbalancer.server.port=8080
    restart: always
